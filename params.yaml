model:
  albert:
    clean_output: true
    model_name: albert-base-v2
    num_labels: 3
    max_len: 192
    batch: 64
    eval_batch: 128
    epochs: 4
    label_smoothing_factor: 0.05
    lr: 2e-5
    weight_decay: 0.05
    seed: 42
    optim: adamw_torch_fused          # <- en 4.56.0 usa adamw_torch (no adamw_hf)
    prefer_cuda: 1
    bf16: true
    fp16: false
    gradient_checkpointing: false
    torch_compile: true
    torch_compile_mode: max-autotune
    tokenize_num_proc: 8    
    eval_strategy: epoch
    save_strategy: epoch
    save_total_limit: 2
    patience: 1
    load_best_model_at_end: true
    text_col: review_text_clean
    label_col: label3           # si no existe, se deriva desde rating
    rating_col: rating
    dataloader_num_workers: 8
    dataloader_prefetch_factor: 4
    dataloader_pin_memory: true
    dataloader_persistent_workers: true
    resume_from_last: false
training:
  class_weights:
    mode: auto
    alpha: 1.0

infer:
  # columnas
  text_col: review_text_clean
  id_col: review_id

  # tamaño de lote y streaming (ajusta solo si te da OOM)
  batch_size: 2048   # si te da OOM baja a 768; si tienes 12GB, prueba 1536
  stream_batch: 100000    # menos overhead de I/O que 50k

  # nada de softmax (más rápido)
  no_probs: true

  # kernels rápidos en GPU
  bf16: true              # RTX 40xx: sí
  fp16: false
  enable_sdp_flash: true
  pad_to_multiple_of: 8

  # compilar la red para inferencia masiva
  torch_compile: true
  torch_compile_mode: reduce-overhead   # usa max-autotune si vas a procesar millones y te compensa el warmup
  torch_compile_backend: inductor

  # tokenizador multi-hilo (Rust HF)
  tokenizers_parallelism: true

  # regla para mejorar precisión de 'neutral' sin reentrenar
  neutral_margin_logit: 0.5

  # mantener longitud
  max_len: 192


mlflow:
  enable: true
  experiment: Sentidata
  tracking_uri: "file:mlruns"     # <- siempre lo normalizará a la raíz del repo
  run_name: albert_sample_30pct   # <- prefijo fijo; el script le añade _YYYYmmdd_HHMMSS
  tags:
    stage: train