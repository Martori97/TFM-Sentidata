#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse, os, json
from pyspark.sql import SparkSession, functions as F

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--mode", choices=["single", "exploitation_all"], default="single",
                   help="single: EDA de una sola fuente. exploitation_all: ejecuta uno o varios EDA de exploitation.")
    p.add_argument("--input", help="Ruta (archivo/carpeta) o patrón (solo en mode=single)")
    p.add_argument("--kind", choices=["csv","parquet"], default="parquet",
                   help="Tipo de input (solo en mode=single). Por defecto parquet en exploitation.")
    p.add_argument("--idcols", nargs="*", default=[], help="Columnas identificadoras (opcional)")
    p.add_argument("--textcols", nargs="*", default=[], help="Columnas de texto (opcional)")
    p.add_argument("--numcols", nargs="*", default=[], help="Columnas numéricas (opcional)")
    p.add_argument("--outdir", default="reports/eda/exploitation_single",
                   help="Carpeta destino (solo en mode=single)")
    # Parámetros para operaciones pesadas
    p.add_argument("--sample_frac_for_tops", type=float, default=0.1,
                   help="Fracción de muestreo para top_values cuando nrows es grande (0<frac<=1).")
    p.add_argument("--sample_threshold_rows", type=int, default=500_000,
                   help="Si filas > threshold, se aplica muestreo para top_values.")
    p.add_argument("--approx_rsd", type=float, default=0.05,
                   help="RSD para approx_count_distinct (0.05 = ~5%).")
    return p.parse_args()

# -------- Spark (sin Delta) con ajustes para Parquet grandes --------
def make_spark():
    """
    SparkSession enfocada a lectura de Parquet/CSV grandes reduciendo riesgo de OOM.
    """
    spark = (
        SparkSession.builder
        .appName("EDA Exploitation Enriched")
        # Lectura Parquet/CSV más conservadora
        .config("spark.sql.parquet.enableVectorizedReader", "false")  # baja presión de heap
        .config("spark.sql.shuffle.partitions", "200")
        .config("spark.sql.files.maxPartitionBytes", str(64 * 1024 * 1024))  # 64MB
        .config("spark.sql.files.openCostInBytes", str(8 * 1024 * 1024))     # 8MB
        # Memoria (ajusta si tu máquina no llega)
        .config("spark.driver.memory", "8g")
        .config("spark.executor.memory", "4g")
        .getOrCreate()
    )
    return spark

# -------- utilidades --------
def read_df(spark, kind, path):
    if kind == "csv":
        return (spark.read
                .option("header", True)
                .option("inferSchema", True)
                .csv(path))
    elif kind == "parquet":
        return spark.read.parquet(path)
    else:
        raise ValueError(f"kind no soportado: {kind}")

def write_csv(df, outpath):
    (df.coalesce(1)
       .write.mode("overwrite")
       .option("header", True)
       .csv(outpath))

def run_eda(spark, input_path, kind, outdir,
            sample_frac_for_tops=0.1, sample_threshold_rows=500_000, approx_rsd=0.05):
    os.makedirs(outdir, exist_ok=True)
    df = read_df(spark, kind, input_path)

    # Resumen
    nrows = df.count()
    schema_list = df.dtypes
    summary = {
        "rows": nrows,
        "columns": len(df.columns),
        "columns_list": [c for c,_ in schema_list],
        "schema": schema_list,
        "source": {"path": input_path, "kind": kind},
        "notes": {
            "top_values_sampled": bool(nrows > sample_threshold_rows and 0 < sample_frac_for_tops < 1.0),
            "approx_distinct_rsd": approx_rsd
        }
    }
    with open(os.path.join(outdir, "summary.json"), "w") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False, default=str)

    # Nulos por columna
    try:
        nulls_row = (df.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in df.columns])
                       .collect()[0].asDict())
    except Exception as e:
        print(f"[EDA Exploitation] Nulls fallo -> {e}")
        nulls_row = {c: None for c in df.columns}
    nulls_rows = [{"column": c, "nulls": (int(nulls_row.get(c, 0)) if nulls_row.get(c) is not None else None),
                   "null_pct": ((float(nulls_row.get(c,0)) / nrows) if (nrows and nulls_row.get(c) is not None) else None)}
                  for c in df.columns]
    write_csv(spark.createDataFrame(nulls_rows), os.path.join(outdir, "nulls_by_column.csv"))

    # Distintos por columna (approx)
    distincts = []
    for c in df.columns:
        try:
            dc = df.agg(F.approx_count_distinct(F.col(c), rsd=approx_rsd).alias("d")).collect()[0]["d"]
        except Exception as e:
            print(f"[EDA Exploitation] approx_count_distinct fallo en {c} -> {e}")
            try:
                dc = df.select(c).distinct().count()
            except Exception:
                dc = None
        distincts.append({"column": c, "distinct_approx": dc, "rsd": approx_rsd})
    write_csv(spark.createDataFrame(distincts), os.path.join(outdir, "distinct_counts_approx.csv"))

    # Top-20 (muestreo si grande)
    string_cols = [c for c,t in schema_list if t == "string"]
    top_dir = os.path.join(outdir, "top_values")
    os.makedirs(top_dir, exist_ok=True)

    if nrows > sample_threshold_rows and 0 < sample_frac_for_tops < 1.0:
        try:
            df_base = df.sample(False, sample_frac_for_tops, seed=42)
        except Exception as e:
            print(f"[EDA Exploitation] sample fallo -> {e}; uso df completo para top_values")
            df_base = df
    else:
        df_base = df

    for c in string_cols:
        try:
            top = (df_base.groupBy(c).count().orderBy(F.desc("count")).limit(20))
            write_csv(top, os.path.join(top_dir, f"{c}.csv"))
        except Exception as e:
            print(f"[EDA Exploitation] top_values fallo en {c} -> {e}")

    # Stats numéricas
    numeric_types = {"int", "bigint", "double", "float", "long", "decimal", "smallint", "short"}
    numeric_cols = [c for c,t in schema_list if t in numeric_types]
    if numeric_cols:
        try:
            stats = (df.select([F.col(c).cast("double").alias(c) for c in numeric_cols])
                       .summary("count","mean","stddev","min","max"))
            write_csv(stats, os.path.join(outdir, "numeric_stats.csv"))
        except Exception as e:
            print(f"[EDA Exploitation] numeric_stats fallo -> {e}")

    # Reglas simples de ejemplo (ajusta si procede)
    rules = []
    if "rating" in df.columns:
        try:
            viol = df.filter(~F.col("rating").between(1,5)).count()
        except Exception as e:
            print(f"[EDA Exploitation] regla rating fallo -> {e}")
            viol = None
        rules.append({"rule": "rating_in_[1,5]", "violations": viol})
    with open(os.path.join(outdir, "rules.json"), "w") as f:
        json.dump(rules, f, indent=2, ensure_ascii=False, default=str)

    print(f"[EDA Exploitation] OK -> rows={nrows}, cols={len(df.columns)}, outdir={outdir}")

# -------- main --------
def main():
    args = parse_args()
    spark = make_spark()

    if args.mode == "single":
        if not args.input:
            raise SystemExit("--input es obligatorio en mode=single")
        run_eda(
            spark, args.input, args.kind, args.outdir,
            sample_frac_for_tops=args.sample_frac_for_tops,
            sample_threshold_rows=args.sample_threshold_rows,
            approx_rsd=args.approx_rsd,
        )

    elif args.mode == "exploitation_all":
        # 1) product_info_snapshot (Parquet dataset directory)
        run_eda(
            spark,
            "data/exploitation/product_info_snapshot.parquet",
            "parquet",
            "reports/eda/exploitation_product_info_snapshot",
        )
        print("[EDA Exploitation All] DONE")

    spark.stop()

if __name__ == "__main__":
    main()

